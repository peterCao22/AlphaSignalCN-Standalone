# è‚¡æ€§ä¸é‡ä»·ç‰¹å¾å®æ–½æ–¹æ¡ˆ

## ğŸ“Š æ•°æ®æ¥æºæ€»è§ˆ

| æ•°æ®è¡¨ | ç”¨é€” | å·²æœ‰/éœ€ä¸‹è½½ |
|--------|------|------------|
| `cn_stock_bar1d` | Kçº¿æ•°æ®ï¼ˆå«turnæ¢æ‰‹ç‡ï¼‰ | âœ… å·²æœ‰ |
| `historical_patterns.db` | å†å²äºŒæ³¢è®°å½• | âœ… å·²æœ‰ |
| `cn_stock_hot_rank` | çƒ­åº¦æ’å | âœ… å·²æœ‰ |
| `cn_stock_index_concept_component` | æ¦‚å¿µæˆåˆ† | âœ… å·²æœ‰ |

**ç»“è®ºï¼šæ‰€æœ‰15ä¸ªç‰¹å¾éƒ½å¯ä»¥ä»ç°æœ‰æ•°æ®è®¡ç®—ï¼Œæ— éœ€é¢å¤–ä¸‹è½½ï¼**

---

## 1ï¸âƒ£ è‚¡æ€§ç‰¹å¾ï¼ˆ7ä¸ªï¼‰- å…¨éƒ¨è‡ªå·±è®¡ç®—

### ç‰¹å¾1: `volatility_60d` - 60æ—¥æ³¢åŠ¨ç‡

**æ•°æ®æ¥æº**ï¼šKçº¿æ•°æ®çš„ `pct_change` å­—æ®µ

**è®¡ç®—å…¬å¼**ï¼š
```python
volatility_60d = df['pct_change'].rolling(60).std()
```

**ç‰©ç†æ„ä¹‰**ï¼š
- å€¼è¶Šå¤§ï¼Œè‚¡ç¥¨æ³¢åŠ¨è¶Šå¤§ï¼Œ"è‚¡æ€§"è¶Šæ´»
- å¸‚åœºå¹³å‡çº¦2.1%
- æ´»è·ƒè‚¡é€šå¸¸ > 2.5%
- å¦–è‚¡é€šå¸¸ > 3.5%

**å®ç°ä½ç½®**ï¼š`enhanced_stock_character_features.py`

---

### ç‰¹å¾2: `limit_up_frequency` - å†å²æ¶¨åœé¢‘ç‡

**æ•°æ®æ¥æº**ï¼šKçº¿æ•°æ®çš„ `is_limit_up` å­—æ®µï¼ˆéœ€è¦å…ˆè®¡ç®—ï¼‰

**è®¡ç®—æ­¥éª¤**ï¼š
```python
# Step 1: è®¡ç®—is_limit_upï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
def calculate_limit_up(df):
    stock_code = df['instrument'].iloc[0].split('.')[0]
    if stock_code.startswith(('92', '43', '8')):
        limit = 29.5  # åŒ—äº¤æ‰€30%
    elif stock_code.startswith(('300', '301', '688', '689')):
        limit = 19.5  # åˆ›ä¸šæ¿/ç§‘åˆ›æ¿20%
    else:
        limit = 9.5   # ä¸»æ¿10%
    df['is_limit_up'] = (df['pct_change'] >= limit).astype(int)
    return df

# Step 2: è®¡ç®—60å¤©æ¶¨åœé¢‘ç‡
limit_up_frequency = df['is_limit_up'].rolling(60).sum() / 60
```

**ç‰©ç†æ„ä¹‰**ï¼š
- 0.10 = 60å¤©å†…6æ¬¡æ¶¨åœï¼ˆå¦–è‚¡ç‰¹å¾ï¼‰
- 0.15 = 60å¤©å†…9æ¬¡æ¶¨åœï¼ˆå¼ºå¦–è‚¡ï¼‰
- > 0.20 = 60å¤©å†…12æ¬¡ä»¥ä¸Šï¼ˆè¶…çº§å¦–è‚¡ï¼‰

---

### ç‰¹å¾3: `second_wave_history` - å†å²äºŒæ³¢æˆåŠŸç‡

**æ•°æ®æ¥æº**ï¼š`historical_patterns.db` æ•°æ®åº“

**è®¡ç®—æ–¹æ³•**ï¼š
```python
import sqlite3

def get_historical_second_wave_rate(symbol, db_path):
    """
    ä»å†å²æ•°æ®åº“ä¸­ç»Ÿè®¡è¯¥è‚¡ç¥¨çš„äºŒæ³¢æˆåŠŸç‡
    """
    conn = sqlite3.connect(db_path)
    query = """
    SELECT 
        COUNT(*) as total_count,
        SUM(CASE WHEN second_wave_confirmed = 1 THEN 1 ELSE 0 END) as success_count
    FROM historical_patterns
    WHERE symbol = ?
    """
    result = pd.read_sql_query(query, conn, params=(symbol,))
    conn.close()
    
    if result['total_count'].iloc[0] > 0:
        return result['success_count'].iloc[0] / result['total_count'].iloc[0]
    else:
        return 0.5  # é»˜è®¤å€¼ï¼ˆå¦‚æœæ²¡æœ‰å†å²è®°å½•ï¼‰
```

**ç‰©ç†æ„ä¹‰**ï¼š
- 0.70 = è¯¥è‚¡å†å²ä¸Š70%çš„æ¶¨åœéƒ½æˆåŠŸäºŒæ³¢
- 0.50 = å†å²ä¸Š50%æˆåŠŸï¼ˆä¸­ç­‰ï¼‰
- < 0.30 = å†å²ä¸Šå¤šæ•°å¤±è´¥ï¼ˆè­¦ç¤ºï¼‰

**æ³¨æ„**ï¼šè¿™æ˜¯ä¸ªè‚¡çº§åˆ«çš„å†å²ç»Ÿè®¡ï¼Œå¸‚åœºæœ‰è®°å¿†ï¼

---

### ç‰¹å¾4: `amplitude_avg_60d` - 60æ—¥å¹³å‡æŒ¯å¹…

**æ•°æ®æ¥æº**ï¼šKçº¿æ•°æ®çš„ `high`, `low`, `close` å­—æ®µ

**è®¡ç®—å…¬å¼**ï¼š
```python
amplitude = (df['high'] - df['low']) / df['close'] * 100
amplitude_avg_60d = amplitude.rolling(60).mean()
```

**ç‰©ç†æ„ä¹‰**ï¼š
- 5.2% = å¸‚åœºå¹³å‡æ°´å¹³
- > 6.5% = æ´»è·ƒè‚¡
- > 8.0% = é«˜é¢‘äº¤æ˜“è‚¡

---

### ç‰¹å¾5: `hot_stock_days` - çƒ­é—¨è‚¡å¤©æ•°

**æ•°æ®æ¥æº**ï¼š`cn_stock_hot_rank` çš„ `hot_rank` å­—æ®µï¼ˆå·²é›†æˆï¼‰

**è®¡ç®—å…¬å¼**ï¼š
```python
# hot_rank <= 100 çš„å¤©æ•°
hot_stock_days = (df['hot_rank'] <= 100).rolling(60).sum()
```

**ç‰©ç†æ„ä¹‰**ï¼š
- 10å¤©ä»¥ä¸Š = å¸‚åœºæŒç»­å…³æ³¨
- 20å¤©ä»¥ä¸Š = è¶…çº§çƒ­é—¨è‚¡
- 0å¤© = å†·é—¨è‚¡

---

### ç‰¹å¾6: `concept_rotation_count` - æ¦‚å¿µè½®åŠ¨æ¬¡æ•°

**æ•°æ®æ¥æº**ï¼š`cn_stock_index_concept_component` çš„ `concept_count` å­—æ®µï¼ˆå·²é›†æˆï¼‰

**è®¡ç®—æ–¹æ³•**ï¼š
```python
# ç»Ÿè®¡60å¤©å†…è¿›å…¥è¿‡å¤šå°‘ä¸ªä¸åŒæ¦‚å¿µ
def calculate_concept_rotation(df):
    # å‡è®¾dfæœ‰concept_listå­—æ®µï¼ˆå¤šä¸ªæ¦‚å¿µç”¨é€—å·åˆ†éš”ï¼‰
    concepts = set()
    for concepts_str in df['concept_list'].tail(60):
        if pd.notna(concepts_str):
            concepts.update(concepts_str.split(','))
    return len(concepts)
```

**ç®€åŒ–ç‰ˆ**ï¼ˆåŸºäºconcept_countï¼‰ï¼š
```python
# ç»Ÿè®¡60å¤©å†…concept_countçš„å³°å€¼ä¸ªæ•°
concept_rotation_count = df['concept_count'].rolling(60).apply(
    lambda x: len(set(x[x > 0]))
)
```

**ç‰©ç†æ„ä¹‰**ï¼š
- 5ä¸ªä»¥ä¸Š = å¤šæ¦‚å¿µè‚¡ï¼Œçƒ­ç‚¹è½®åŠ¨å¿«
- 2-3ä¸ª = ç¨³å®šæ¦‚å¿µ
- 1ä¸ª = å•ä¸€æ¦‚å¿µ

---

### ç‰¹å¾7: `rebound_speed` - å›è°ƒåå¼¹é€Ÿåº¦

**æ•°æ®æ¥æº**ï¼šKçº¿æ•°æ®

**è®¡ç®—æ–¹æ³•**ï¼š
```python
def calculate_rebound_speed(df):
    """
    è®¡ç®—å¹³å‡åå¼¹å¤©æ•°ï¼šä»å›è°ƒä½ç‚¹åˆ°å†æ¬¡æ¶¨åœçš„å¤©æ•°
    """
    speeds = []
    is_limit_up = df['is_limit_up'].values
    
    for i in range(1, len(df)):
        if is_limit_up[i] == 1:  # å½“å¤©æ¶¨åœ
            # å‘å‰æŸ¥æ‰¾ä¸Šä¸€ä¸ªæ¶¨åœ
            for j in range(i-1, max(0, i-30), -1):
                if is_limit_up[j] == 1:
                    speeds.append(i - j)
                    break
    
    return np.mean(speeds) if speeds else 10  # é»˜è®¤10å¤©
```

**ç‰©ç†æ„ä¹‰**ï¼š
- < 5å¤© = å¿«é€Ÿåå¼¹ï¼Œå¼ºåŠ¿
- 5-10å¤© = æ­£å¸¸
- > 15å¤© = åå¼¹æ…¢ï¼Œå¼±åŠ¿

---

## 2ï¸âƒ£ é‡ä»·é…åˆç‰¹å¾ï¼ˆ8ä¸ªï¼‰- å…¨éƒ¨è‡ªå·±è®¡ç®—

### ç‰¹å¾8: `volume_price_correlation` - é‡ä»·ç›¸å…³æ€§

**æ•°æ®æ¥æº**ï¼šKçº¿æ•°æ®çš„ `volume` å’Œ `close` å­—æ®µ

**è®¡ç®—å…¬å¼**ï¼š
```python
volume_price_correlation = df['volume'].rolling(20).corr(df['close'])
```

**ç‰©ç†æ„ä¹‰**ï¼š
- > 0.7 = é‡ä»·åŒæ­¥ï¼Œå¥åº·ä¸Šæ¶¨
- 0.3-0.7 = ä¸­ç­‰
- < 0.3 = é‡ä»·èƒŒç¦»ï¼Œè­¦ç¤ºä¿¡å·ï¼

---

### ç‰¹å¾9: `volume_increase_ratio` - æ”¾é‡å€æ•°

**æ•°æ®æ¥æº**ï¼šKçº¿æ•°æ®çš„ `volume` å­—æ®µ

**è®¡ç®—å…¬å¼**ï¼š
```python
volume_ma20 = df['volume'].rolling(20).mean()
volume_increase_ratio = df['volume'] / volume_ma20
```

**ç‰©ç†æ„ä¹‰**ï¼š
- > 2.0 = æ”¾é‡çªç ´
- 1.5-2.0 = æ¸©å’Œæ”¾é‡
- < 1.0 = ç¼©é‡

---

### ç‰¹å¾10: `volume_price_divergence` - é‡ä»·èƒŒç¦»

**æ•°æ®æ¥æº**ï¼šKçº¿æ•°æ®çš„ `volume` å’Œ `close` å­—æ®µ

**è®¡ç®—æ–¹æ³•**ï¼š
```python
# ä»·æ ¼è¶‹åŠ¿ï¼ˆæ¶¨=1ï¼Œè·Œ=0ï¼‰
price_trend = (df['close'] > df['close'].shift(1)).astype(int)

# æˆäº¤é‡è¶‹åŠ¿ï¼ˆå¢=1ï¼Œå‡=0ï¼‰
volume_trend = (df['volume'] > df['volume'].shift(1)).astype(int)

# èƒŒç¦»æ£€æµ‹ï¼ˆè¶‹åŠ¿ä¸ä¸€è‡´ï¼‰
volume_price_divergence = (price_trend != volume_trend).astype(int)
```

**ç‰©ç†æ„ä¹‰**ï¼š
- 1 = èƒŒç¦»ï¼ˆä»·æ¶¨é‡ç¼© æˆ– ä»·è·Œé‡å¢ï¼‰
- 0 = åŒæ­¥

---

### ç‰¹å¾11: `shrink_limit_up` - ç¼©é‡æ¶¨åœ

**æ•°æ®æ¥æº**ï¼šKçº¿æ•°æ®çš„ `volume` å’Œ `is_limit_up` å­—æ®µ

**è®¡ç®—å…¬å¼**ï¼š
```python
volume_ma5 = df['volume'].rolling(5).mean()
shrink_limit_up = (df['is_limit_up'] == 1) & (df['volume'] < volume_ma5)
shrink_limit_up = shrink_limit_up.astype(int)
```

**ç‰©ç†æ„ä¹‰**ï¼š
- 1 = ç¼©é‡æ¶¨åœï¼ˆé£é™©å¤§ï¼Œå®¹æ˜“å¼€æ¿ï¼‰
- 0 = æ”¾é‡æ¶¨åœï¼ˆå¥åº·ï¼‰

---

### ç‰¹å¾12: `turnover_rate` - æ¢æ‰‹ç‡

**æ•°æ®æ¥æº**ï¼šBigQuant `cn_stock_bar1d` çš„ `turn` å­—æ®µ

**è·å–æ–¹å¼**ï¼š
```python
# ä»Kçº¿æ•°æ®ç›´æ¥è¯»å–ï¼ˆBigQuantå·²è®¡ç®—ï¼‰
turnover_rate = df['turn']  # å·²åœ¨kline_all.csvä¸­
```

**ç‰©ç†æ„ä¹‰**ï¼š
- > 15% = é«˜æ¢æ‰‹ï¼Œèµ„é‡‘æ´»è·ƒ
- 8-15% = ä¸­ç­‰
- < 5% = ä½æ¢æ‰‹ï¼Œèµ„é‡‘ä¸æ´»è·ƒ

**æ³¨æ„**ï¼šè¿™ä¸ªå­—æ®µåœ¨ `cn_stock_bar1d` ä¸­å·²ç»å­˜åœ¨ï¼Œç›´æ¥ç”¨ï¼

---

### ç‰¹å¾13: `volume_continuity` - é‡èƒ½æŒç»­æ€§

**æ•°æ®æ¥æº**ï¼šKçº¿æ•°æ®çš„ `volume` å­—æ®µ

**è®¡ç®—æ–¹æ³•**ï¼š
```python
def calculate_volume_continuity(df):
    """
    è®¡ç®—è¿ç»­æ”¾é‡å¤©æ•°
    """
    volume_ma5 = df['volume'].rolling(5).mean()
    is_high_volume = (df['volume'] > volume_ma5).astype(int)
    
    # è®¡ç®—è¿ç»­å¤©æ•°
    continuity = []
    count = 0
    for val in is_high_volume:
        if val == 1:
            count += 1
        else:
            count = 0
        continuity.append(count)
    
    return continuity
```

**ç‰©ç†æ„ä¹‰**ï¼š
- >= 3å¤© = æŒç»­æ”¾é‡ï¼Œå¼ºåŠ¿
- 1-2å¤© = çŸ­æœŸæ”¾é‡
- 0å¤© = ç¼©é‡

---

### ç‰¹å¾14: `volume_pattern` - é‡èƒ½å½¢æ€

**æ•°æ®æ¥æº**ï¼šKçº¿æ•°æ®çš„ `volume` å­—æ®µ

**è®¡ç®—æ–¹æ³•**ï¼š
```python
def calculate_volume_pattern(df):
    """
    é‡èƒ½å½¢æ€è¯†åˆ«ï¼šé€’å¢(1)ã€é€’å‡(-1)ã€éœ‡è¡(0)
    """
    volume_5d = df['volume'].rolling(5).mean()
    volume_20d = df['volume'].rolling(20).mean()
    
    # é€’å¢ï¼š5æ—¥å‡çº¿æŒç»­ä¸Šç©¿20æ—¥å‡çº¿
    if (volume_5d.iloc[-1] > volume_20d.iloc[-1]) and \
       (volume_5d.iloc[-2] > volume_20d.iloc[-2]):
        return 1  # é€’å¢
    
    # é€’å‡ï¼š5æ—¥å‡çº¿æŒç»­ä¸‹ç©¿20æ—¥å‡çº¿
    elif (volume_5d.iloc[-1] < volume_20d.iloc[-1]) and \
         (volume_5d.iloc[-2] < volume_20d.iloc[-2]):
        return -1  # é€’å‡
    
    else:
        return 0  # éœ‡è¡
```

**ç‰©ç†æ„ä¹‰**ï¼š
- 1 = é€’å¢ï¼ˆé‡èƒ½å †ç§¯ï¼Œçœ‹æ¶¨ï¼‰
- 0 = éœ‡è¡ï¼ˆæ•´ç†ï¼‰
- -1 = é€’å‡ï¼ˆé‡èƒ½è¡°å‡ï¼Œçœ‹è·Œï¼‰

---

### ç‰¹å¾15: `volume_ma_ratio` - é‡èƒ½å‡çº¿æ¯”

**æ•°æ®æ¥æº**ï¼šKçº¿æ•°æ®çš„ `volume` å­—æ®µ

**è®¡ç®—å…¬å¼**ï¼š
```python
volume_ma5 = df['volume'].rolling(5).mean()
volume_ma20 = df['volume'].rolling(20).mean()
volume_ma_ratio = volume_ma5 / volume_ma20
```

**ç‰©ç†æ„ä¹‰**ï¼š
- > 1.2 = çŸ­æœŸé‡èƒ½å¼ºäºé•¿æœŸ
- 0.8-1.2 = å¹³è¡¡
- < 0.8 = çŸ­æœŸé‡èƒ½å¼±äºé•¿æœŸ

---

## ğŸš€ å®æ–½æ­¥éª¤

### Step 1: åˆ›å»ºç‰¹å¾å·¥ç¨‹è„šæœ¬

åˆ›å»º `scripts/enhanced_stock_character_features.py`

### Step 2: æ•°æ®å‡†å¤‡

```python
import pandas as pd
import numpy as np
import sqlite3
from pathlib import Path

# è¯»å–Kçº¿æ•°æ®
kline_df = pd.read_csv('data/raw/kline/kline_all.csv')
kline_df['date'] = pd.to_datetime(kline_df['date'], format='ISO8601')

# è¯»å–çƒ­åº¦æ•°æ®
hot_rank_df = pd.read_csv('data/raw/hot_rank.csv')
hot_rank_df['date'] = pd.to_datetime(hot_rank_df['date'], format='ISO8601')

# è¯»å–æ¦‚å¿µæ•°æ®
concept_df = pd.read_csv('data/raw/concept_component.csv')
concept_df['date'] = pd.to_datetime(concept_df['date'], format='ISO8601')

# è¿æ¥å†å²æ•°æ®åº“
db_conn = sqlite3.connect('data/historical_patterns.db')
```

### Step 3: é€ä¸ªè®¡ç®—ç‰¹å¾

```python
def calculate_all_features(kline_df, hot_rank_df, concept_df, db_path):
    """
    è®¡ç®—æ‰€æœ‰15ä¸ªæ–°ç‰¹å¾
    """
    result_df = kline_df.copy()
    
    # æŒ‰è‚¡ç¥¨åˆ†ç»„è®¡ç®—
    for symbol, group in result_df.groupby('instrument'):
        group = group.sort_values('date')
        
        # === è‚¡æ€§ç‰¹å¾ ===
        # 1. æ³¢åŠ¨ç‡
        group['volatility_60d'] = group['pct_change'].rolling(60).std()
        
        # 2. æ¶¨åœé¢‘ç‡
        group = calculate_limit_up(group)
        group['limit_up_frequency'] = group['is_limit_up'].rolling(60).sum() / 60
        
        # 3. å†å²äºŒæ³¢æˆåŠŸç‡ï¼ˆä»æ•°æ®åº“æŸ¥è¯¢ï¼‰
        group['second_wave_history'] = get_historical_second_wave_rate(
            symbol, db_path
        )
        
        # 4. å¹³å‡æŒ¯å¹…
        amplitude = (group['high'] - group['low']) / group['close'] * 100
        group['amplitude_avg_60d'] = amplitude.rolling(60).mean()
        
        # 5. çƒ­é—¨è‚¡å¤©æ•°ï¼ˆéœ€è¦åˆå¹¶hot_rank_dfï¼‰
        group = pd.merge(group, hot_rank_df[['instrument', 'date', 'rank']], 
                         on=['instrument', 'date'], how='left')
        group['hot_stock_days'] = (group['rank'] <= 100).rolling(60).sum()
        
        # 6. æ¦‚å¿µè½®åŠ¨æ¬¡æ•°
        group = pd.merge(group, concept_df[['member_code', 'date']], 
                         left_on=['instrument', 'date'],
                         right_on=['member_code', 'date'], how='left')
        group['concept_rotation_count'] = calculate_concept_rotation(group)
        
        # 7. å›è°ƒåå¼¹é€Ÿåº¦
        group['rebound_speed'] = calculate_rebound_speed(group)
        
        # === é‡ä»·é…åˆç‰¹å¾ ===
        # 8. é‡ä»·ç›¸å…³æ€§
        group['volume_price_correlation'] = group['volume'].rolling(20).corr(
            group['close']
        )
        
        # 9. æ”¾é‡å€æ•°
        volume_ma20 = group['volume'].rolling(20).mean()
        group['volume_increase_ratio'] = group['volume'] / volume_ma20
        
        # 10. é‡ä»·èƒŒç¦»
        price_trend = (group['close'] > group['close'].shift(1)).astype(int)
        volume_trend = (group['volume'] > group['volume'].shift(1)).astype(int)
        group['volume_price_divergence'] = (price_trend != volume_trend).astype(int)
        
        # 11. ç¼©é‡æ¶¨åœ
        volume_ma5 = group['volume'].rolling(5).mean()
        group['shrink_limit_up'] = (
            (group['is_limit_up'] == 1) & (group['volume'] < volume_ma5)
        ).astype(int)
        
        # 12. æ¢æ‰‹ç‡ï¼ˆç›´æ¥ä»Kçº¿æ•°æ®è¯»å–ï¼‰
        group['turnover_rate'] = group['turn']  # BigQuantå·²è®¡ç®—
        
        # 13. é‡èƒ½æŒç»­æ€§
        group['volume_continuity'] = calculate_volume_continuity(group)
        
        # 14. é‡èƒ½å½¢æ€
        group['volume_pattern'] = calculate_volume_pattern(group)
        
        # 15. é‡èƒ½å‡çº¿æ¯”
        volume_ma5 = group['volume'].rolling(5).mean()
        volume_ma20 = group['volume'].rolling(20).mean()
        group['volume_ma_ratio'] = volume_ma5 / volume_ma20
        
        # æ›´æ–°ç»“æœ
        result_df.loc[group.index] = group
    
    return result_df
```

### Step 4: ä¿å­˜ç‰¹å¾æ•°æ®

```python
# ä¿å­˜å®Œæ•´ç‰¹å¾æ•°æ®ï¼ˆæ¨èï¼šè¾“å‡ºåˆ°ç‹¬ç«‹ç›®å½• + æ—¶é—´æˆ³æ–‡ä»¶åï¼Œé¿å…è¢«è¡¥Kçº¿é€»è¾‘è¯¯é€‰ä¸­/è¦†ç›–ï¼‰
from datetime import datetime

timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
output_path = f"data/raw/features/features_kline_stock_character_{timestamp}.csv"
result_df.to_csv(output_path, index=False)
print(f"âœ“ ç‰¹å¾æ•°æ®å·²ä¿å­˜è‡³: {output_path}")
```

---

## ğŸ“Š æ•°æ®æµæ€»ç»“

```
ç°æœ‰æ•°æ®æº:
â”œâ”€ data/raw/kline/kline_all.csv          â†’ Kçº¿æ•°æ®ï¼ˆå«turnæ¢æ‰‹ç‡ï¼‰
â”œâ”€ data/raw/hot_rank.csv                 â†’ çƒ­åº¦æ’å
â”œâ”€ data/raw/concept_component.csv        â†’ æ¦‚å¿µæˆåˆ†
â””â”€ data/historical_patterns.db           â†’ å†å²äºŒæ³¢è®°å½•

         â†“ ç‰¹å¾å·¥ç¨‹ï¼ˆå…¨éƒ¨è‡ªå·±è®¡ç®—ï¼‰

æ–°ç‰¹å¾æ•°æ®:
â””â”€ data/raw/features/features_kline_stock_character_YYYYMMDD_HHMMSS.csv
   â”œâ”€ [åŸæœ‰å­—æ®µ]
   â”œâ”€ volatility_60d                     âœ“ è‡ªå·±è®¡ç®—
   â”œâ”€ limit_up_frequency                 âœ“ è‡ªå·±è®¡ç®—
   â”œâ”€ second_wave_history                âœ“ ä»æ•°æ®åº“ç»Ÿè®¡
   â”œâ”€ amplitude_avg_60d                  âœ“ è‡ªå·±è®¡ç®—
   â”œâ”€ hot_stock_days                     âœ“ è‡ªå·±è®¡ç®—
   â”œâ”€ concept_rotation_count             âœ“ è‡ªå·±è®¡ç®—
   â”œâ”€ rebound_speed                      âœ“ è‡ªå·±è®¡ç®—
   â”œâ”€ volume_price_correlation           âœ“ è‡ªå·±è®¡ç®—
   â”œâ”€ volume_increase_ratio              âœ“ è‡ªå·±è®¡ç®—
   â”œâ”€ volume_price_divergence            âœ“ è‡ªå·±è®¡ç®—
   â”œâ”€ shrink_limit_up                    âœ“ è‡ªå·±è®¡ç®—
   â”œâ”€ turnover_rate                      âœ“ BigQuantåŸå­—æ®µ
   â”œâ”€ volume_continuity                  âœ“ è‡ªå·±è®¡ç®—
   â”œâ”€ volume_pattern                     âœ“ è‡ªå·±è®¡ç®—
   â””â”€ volume_ma_ratio                    âœ“ è‡ªå·±è®¡ç®—

         â†“ é›†æˆåˆ°è®­ç»ƒæµç¨‹

scripts/train_model.py:
â”œâ”€ åŠ è½½ data/raw/features/features_kline_stock_character_YYYYMMDD_HHMMSS.csvï¼ˆé€‰æ‹©æœ€æ–°ç”Ÿæˆçš„æ–‡ä»¶ï¼‰
â”œâ”€ æ›´æ–° feature_colsï¼ˆ83 â†’ 98ï¼‰
â””â”€ é‡æ–°è®­ç»ƒæ¨¡å‹
```

---

## â±ï¸ å¼€å‘æ—¶é—´ä¼°ç®—

| æ­¥éª¤ | æ—¶é—´ | éš¾åº¦ |
|------|------|------|
| ç¼–å†™ç‰¹å¾è®¡ç®—å‡½æ•° | 1.5å°æ—¶ | â­â­ |
| æ•°æ®åˆå¹¶å’Œè°ƒè¯• | 1å°æ—¶ | â­â­ |
| é›†æˆåˆ°è®­ç»ƒæµç¨‹ | 0.5å°æ—¶ | â­ |
| æ¨¡å‹è®­ç»ƒ | 1å°æ—¶ | â­ |
| **æ€»è®¡** | **4å°æ—¶** | **â­â­** |

---

## ğŸ¯ å…³é”®ä¼˜åŠ¿

1. **æ— éœ€é¢å¤–ä¸‹è½½æ•°æ®** âœ…
   - æ‰€æœ‰ç‰¹å¾éƒ½ä»ç°æœ‰Kçº¿æ•°æ®è®¡ç®—
   - å”¯ä¸€å¤–éƒ¨å­—æ®µ `turn`ï¼ˆæ¢æ‰‹ç‡ï¼‰å·²åœ¨BigQuant Kçº¿ä¸­

2. **è®¡ç®—æ•ˆç‡é«˜** âœ…
   - ä½¿ç”¨pandaså‘é‡åŒ–æ“ä½œ
   - rollingè®¡ç®—é«˜æ•ˆ
   - é¢„è®¡å¤„ç†3000åªè‚¡ç¥¨Ã—732å¤© < 10åˆ†é’Ÿ

3. **å¯è§£é‡Šæ€§å¼º** âœ…
   - æ¯ä¸ªç‰¹å¾éƒ½æœ‰æ˜ç¡®ç‰©ç†æ„ä¹‰
   - ä¾¿äºå›æµ‹å’Œè°ƒè¯•

---

## ğŸš¨ æ³¨æ„äº‹é¡¹

### 1. æ¢æ‰‹ç‡å­—æ®µæ£€æŸ¥

```python
# æ£€æŸ¥kline_all.csvæ˜¯å¦æœ‰turnå­—æ®µ
import pandas as pd
df = pd.read_csv('data/raw/kline/kline_all.csv', nrows=10)
print(df.columns.tolist())

# å¦‚æœæ²¡æœ‰turnå­—æ®µï¼Œéœ€è¦é‡æ–°ä¸‹è½½Kçº¿æ•°æ®ï¼Œæˆ–è€…ä»BigQuantå•ç‹¬è·å–
```

### 2. historical_patterns.db æ•°æ®é‡

```python
# æ£€æŸ¥æ•°æ®åº“ä¸­æœ‰å¤šå°‘å†å²è®°å½•
import sqlite3
conn = sqlite3.connect('data/historical_patterns.db')
df = pd.read_sql_query("SELECT COUNT(*) FROM historical_patterns", conn)
print(f"å†å²è®°å½•æ•°: {df.iloc[0, 0]}")
conn.close()

# å¦‚æœè®°å½•å°‘äº5000æ¡ï¼Œsecond_wave_historyç‰¹å¾çš„å¯é æ€§ä¼šé™ä½
```

### 3. å†…å­˜å ç”¨

```python
# å¦‚æœæ•°æ®é‡å¤§ï¼ˆ>3GBï¼‰ï¼Œå»ºè®®åˆ†æ‰¹å¤„ç†
chunk_size = 500  # æ¯æ¬¡å¤„ç†500åªè‚¡ç¥¨
for i in range(0, len(stock_list), chunk_size):
    chunk = stock_list[i:i+chunk_size]
    process_chunk(chunk)
```

---

## ä¸‹ä¸€æ­¥ï¼šå¼€å§‹å®æ–½ï¼Ÿ

å‡†å¤‡å¥½åï¼Œæˆ‘å°†åˆ›å»ºå®Œæ•´çš„ `enhanced_stock_character_features.py` è„šæœ¬ï¼
