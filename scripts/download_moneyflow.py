"""
ä¸‹è½½èµ„é‡‘æµå‘æ•°æ®ï¼ˆç²¾é€‰15ä¸ªå…³é”®å­—æ®µ - ä¼˜åŒ–ç‰ˆï¼‰

ç”¨äºäºŒæ³¢é¢„æµ‹çš„æ ¸å¿ƒèµ„é‡‘æµæŒ‡æ ‡ï¼š
- å‡€æµå…¥é‡‘é¢ï¼ˆä¸»åŠ›ã€è¶…å¤§å•ã€å¤§å•ã€å°å•ï¼‰- å·²é¢„è®¡ç®— âœ“
- å‡€ä¸»åŠ¨ä¹°å…¥é¢ï¼ˆä¸»åŠ›ã€è¶…å¤§å•ï¼‰- å·²é¢„è®¡ç®— âœ“
- èµ„é‡‘æµå…¥/æµå‡ºå æ¯” - å·²é¢„è®¡ç®— âœ“
- åŸå§‹æµå…¥/æµå‡ºé‡‘é¢ï¼ˆç”¨äºè¡ç”Ÿè®¡ç®—ï¼‰

ä¼˜åŠ¿ï¼šä½¿ç”¨ BigQuant é¢„è®¡ç®—å­—æ®µï¼Œæ— éœ€æ‰‹åŠ¨è®¡ç®—å‡€å€¼ï¼Œæ›´å‡†ç¡®é«˜æ•ˆ

ä½œè€…ï¼šAI Assistant
æ—¥æœŸï¼š2026-01-14
"""

import os
import sys
import pandas as pd
import logging
from datetime import datetime, timedelta

# ç»Ÿä¸€ä»¥æœ¬ä»“åº“æ ¹ç›®å½•ä¸ºå‡†ï¼ˆAlphaSignalCN-Standaloneï¼‰
from pathlib import Path

REPO_ROOT = Path(__file__).resolve().parent.parent

# å¯¼å…¥ BigQuant SDK
try:
    from bigquantdai import dai
except ImportError:
    try:
        from bigquant.api import dai
    except ImportError:
        import dai

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)


# ç²¾é€‰çš„15ä¸ªå…³é”®å­—æ®µï¼ˆä¼˜åŒ–ç‰ˆ - ä½¿ç”¨é¢„è®¡ç®—å­—æ®µï¼‰
KEY_MONEYFLOW_FIELDS = [
    # åŸºç¡€å­—æ®µ
    'date',
    'instrument',
    
    # ã€æœ€æ ¸å¿ƒã€‘å‡€æµå…¥é‡‘é¢ï¼ˆå·²é¢„è®¡ç®—ï¼Œç›´æ¥å¯ç”¨ï¼‰
    'netflow_amount_main',              # ä¸»åŠ›å‡€æµå…¥é‡‘é¢
    'netflow_amount_large',             # è¶…å¤§å•å‡€æµå…¥é‡‘é¢
    'netflow_amount_big',               # å¤§å•å‡€æµå…¥é‡‘é¢
    'netflow_amount_small',             # å°å•å‡€æµå…¥é‡‘é¢ï¼ˆæ•£æˆ·ï¼‰
    
    # ã€æ¬¡æ ¸å¿ƒã€‘å‡€ä¸»åŠ¨ä¹°å…¥ï¼ˆå¦ä¸€ç§ç®—æ³•ï¼‰
    'net_active_buy_amount_main',       # ä¸»åŠ›å‡€ä¸»åŠ¨ä¹°å…¥é¢
    'net_active_buy_amount_large',      # è¶…å¤§å•å‡€ä¸»åŠ¨ä¹°å…¥é¢
    
    # ã€é‡è¦ã€‘èµ„é‡‘æµå…¥å æ¯”ï¼ˆå·²é¢„è®¡ç®—ï¼‰
    'netflow_amount_rate_main',         # ä¸»åŠ›å‡€æµå…¥å æ¯”
    'inflow_amount_rate_main',          # ä¸»åŠ›æµå…¥å æ¯”
    'outflow_amount_rate_main',         # ä¸»åŠ›æµå‡ºå æ¯”
    
    # ã€è¾…åŠ©ã€‘ç»å¯¹å€¼ï¼ˆç”¨äºè®¡ç®—å…¶ä»–æŒ‡æ ‡ï¼‰
    'inflow_amount_main',               # ä¸»åŠ›æµå…¥é‡‘é¢
    'outflow_amount_main',              # ä¸»åŠ›æµå‡ºé‡‘é¢
    
    # ã€åŸºå‡†ã€‘å…¨å•æ±‡æ€»
    'active_buy_amount_all',            # å…¨éƒ¨ä¸»åŠ¨ä¹°å…¥é¢ï¼ˆä½œä¸ºè®¡ç®—åŸºå‡†ï¼‰
]


def download_moneyflow_batch(start_date, end_date):
    """
    ä¸‹è½½å•ä¸ªæ‰¹æ¬¡çš„èµ„é‡‘æµå‘æ•°æ®
    
    Args:
        start_date: å¼€å§‹æ—¥æœŸ
        end_date: ç»“æŸæ—¥æœŸ
    
    Returns:
        DataFrame æˆ– None
    """
    fields_str = ', '.join(KEY_MONEYFLOW_FIELDS)
    sql = f"""
    SELECT {fields_str}
    FROM cn_stock_moneyflow
    """
    
    try:
        logging.info(f"  ä¸‹è½½æ‰¹æ¬¡: {start_date} ~ {end_date}")
        df = dai.query(sql, filters={"date": [start_date, end_date]}).df()
        
        if df is None or df.empty:
            logging.warning(f"  è¯¥æ‰¹æ¬¡æ— æ•°æ®")
            return None
        
        df['date'] = pd.to_datetime(df['date'])
        logging.info(f"  âœ“ è·å– {len(df):,} æ¡æ•°æ®")
        return df
        
    except Exception as e:
        logging.error(f"  âŒ è¯¥æ‰¹æ¬¡ä¸‹è½½å¤±è´¥: {e}")
        return None


def download_moneyflow(days_range=365, output_dir='data/raw', batch_days=60):
    """
    ä¸‹è½½èµ„é‡‘æµå‘æ•°æ®ï¼ˆåˆ†æ‰¹ä¸‹è½½ä»¥é¿å… 200MB é™åˆ¶ï¼‰
    
    Args:
        days_range: ä¸‹è½½å¤©æ•°èŒƒå›´ï¼ˆé»˜è®¤365å¤©ï¼‰
        output_dir: è¾“å‡ºç›®å½•
        batch_days: æ¯æ‰¹ä¸‹è½½å¤©æ•°ï¼ˆé»˜è®¤60å¤©ï¼Œé¿å…è¶…è¿‡200MBé™åˆ¶ï¼‰
    """
    
    out_dir = Path(output_dir)
    if not out_dir.is_absolute():
        out_dir = REPO_ROOT / out_dir
    out_dir.mkdir(parents=True, exist_ok=True)
    output_file = str(out_dir / "moneyflow.csv")
    
    # è®¡ç®—æ—¥æœŸèŒƒå›´
    end_date = datetime.now()
    start_date = end_date - timedelta(days=days_range)
    
    logging.info("=" * 80)
    logging.info("å¼€å§‹ä¸‹è½½èµ„é‡‘æµå‘æ•°æ®ï¼ˆåˆ†æ‰¹ä¸‹è½½ï¼‰")
    logging.info("=" * 80)
    logging.info(f"å­—æ®µæ•°: {len(KEY_MONEYFLOW_FIELDS)} ä¸ªï¼ˆç²¾é€‰å…³é”®æŒ‡æ ‡ï¼‰")
    logging.info(f"æ—¥æœŸèŒƒå›´: {start_date.strftime('%Y-%m-%d')} ~ {end_date.strftime('%Y-%m-%d')} ({days_range} å¤©)")
    logging.info(f"æ‰¹æ¬¡å¤§å°: {batch_days} å¤©/æ‰¹ï¼ˆé¿å… 200MB é™åˆ¶ï¼‰")
    logging.info(f"è¾“å‡ºæ–‡ä»¶: {output_file}")
    
    # æ£€æŸ¥æ˜¯å¦å·²æœ‰æ•°æ®ï¼ˆå¢é‡ä¸‹è½½ï¼‰
    existing_df = None
    latest_date = None
    
    if os.path.exists(output_file):
        try:
            existing_df = pd.read_csv(output_file)
            existing_df['date'] = pd.to_datetime(existing_df['date'])
            latest_date = existing_df['date'].max()
            logging.info(f"\nâœ“ å‘ç°å·²æœ‰æ•°æ®: {len(existing_df)} æ¡")
            logging.info(f"æœ€æ–°æ—¥æœŸ: {latest_date.strftime('%Y-%m-%d')}")
            
            # è°ƒæ•´å¼€å§‹æ—¥æœŸï¼ˆåªä¸‹è½½æ–°æ•°æ®ï¼‰
            if latest_date and latest_date >= start_date:
                start_date = latest_date + timedelta(days=1)
                logging.info(f"è°ƒæ•´ä¸ºå¢é‡ä¸‹è½½: {start_date.strftime('%Y-%m-%d')} ~ {end_date.strftime('%Y-%m-%d')}")
                
                if start_date >= end_date:
                    logging.info("\nâœ“ æ•°æ®å·²æ˜¯æœ€æ–°ï¼Œæ— éœ€ä¸‹è½½")
                    return existing_df
        except Exception as e:
            logging.warning(f"è¯»å–å·²æœ‰æ•°æ®å¤±è´¥: {e}ï¼Œå°†å…¨é‡ä¸‹è½½")
            existing_df = None
    
    # åˆ†æ‰¹ä¸‹è½½
    all_batches = []
    current_start = start_date
    batch_num = 0
    
    logging.info(f"\nå¼€å§‹åˆ†æ‰¹ä¸‹è½½...")
    
    while current_start < end_date:
        batch_num += 1
        current_end = min(current_start + timedelta(days=batch_days), end_date)
        
        logging.info(f"\nã€æ‰¹æ¬¡ {batch_num}ã€‘")
        batch_df = download_moneyflow_batch(
            current_start.strftime('%Y-%m-%d'),
            current_end.strftime('%Y-%m-%d')
        )
        
        if batch_df is not None:
            all_batches.append(batch_df)
        
        current_start = current_end + timedelta(days=1)
    
    if not all_batches:
        logging.warning("\næœªè·å–åˆ°ä»»ä½•æ•°æ®")
        return existing_df if existing_df is not None else pd.DataFrame()
    
    # åˆå¹¶æ‰€æœ‰æ‰¹æ¬¡
    logging.info(f"\nåˆå¹¶ {len(all_batches)} ä¸ªæ‰¹æ¬¡...")
    df = pd.concat(all_batches, ignore_index=True)
    
    logging.info(f"\nâœ“ ä¸‹è½½æˆåŠŸ: {len(df)} æ¡æ–°æ•°æ®")
    logging.info(f"æ—¥æœŸèŒƒå›´: {df['date'].min()} ~ {df['date'].max()}")
    logging.info(f"è‚¡ç¥¨æ•°é‡: {df['instrument'].nunique()} åª")
    
    # æ•°æ®ç»Ÿè®¡
    logging.info(f"\næ•°æ®ç»Ÿè®¡:")
    logging.info(f"  - æ€»è®°å½•æ•°: {len(df):,}")
    logging.info(f"  - å­—æ®µæ•°: {len(df.columns)}")
    logging.info(f"  - æ•°æ®å¤§å°: {df.memory_usage(deep=True).sum() / 1024 / 1024:.2f} MB")
    
    # åˆå¹¶æ–°æ—§æ•°æ®
    if existing_df is not None:
        logging.info(f"\nåˆå¹¶å·²æœ‰æ•°æ®...")
        df = pd.concat([existing_df, df], ignore_index=True)
        logging.info(f"å»é‡å‰: {len(df)} æ¡")
        
        # å»é‡ï¼ˆä¿ç•™æœ€æ–°çš„è®°å½•ï¼‰
        df = df.drop_duplicates(subset=['date', 'instrument'], keep='last')
        logging.info(f"å»é‡å: {len(df)} æ¡")
    
    # æŒ‰æ—¥æœŸæ’åº
    df = df.sort_values(['date', 'instrument']).reset_index(drop=True)
    
    # ä¿å­˜æ•°æ®
    df.to_csv(output_file, index=False)
    logging.info(f"\nâœ“ å·²ä¿å­˜: {output_file}")
    
    # å­—æ®µæœ‰æ•ˆæ€§ç»Ÿè®¡
    logging.info(f"\nå­—æ®µæœ‰æ•ˆæ€§:")
    for col in KEY_MONEYFLOW_FIELDS[2:]:  # è·³è¿‡ date å’Œ instrument
        valid_pct = (df[col].notna() & (df[col] != 0)).sum() / len(df) * 100
        logging.info(f"  - {col:35s}: {valid_pct:5.1f}% æœ‰æ•ˆ")
    
    # æ•°æ®è´¨é‡æ£€æŸ¥
    logging.info(f"\nğŸ’¡ æ ¸å¿ƒæŒ‡æ ‡ç»Ÿè®¡:")
    logging.info(f"  - netflow_amount_mainï¼ˆä¸»åŠ›å‡€æµå…¥ï¼‰: å·²é¢„è®¡ç®— âœ“")
    logging.info(f"  - netflow_amount_rate_mainï¼ˆä¸»åŠ›å‡€æµå…¥å æ¯”ï¼‰: å·²é¢„è®¡ç®— âœ“")
    logging.info(f"  - inflow/outflow_amount_rateï¼ˆæµå…¥/æµå‡ºå æ¯”ï¼‰: å·²é¢„è®¡ç®— âœ“")
    
    # ç¤ºä¾‹ç»Ÿè®¡
    positive_count = (df['netflow_amount_main'] > 0).sum()
    logging.info(f"\nèµ„é‡‘æµå‘ç»Ÿè®¡:")
    logging.info(f"  - ä¸»åŠ›å‡€æµå…¥æ ·æœ¬: {positive_count:,} ({positive_count/len(df)*100:.1f}%)")
    logging.info(f"  - ä¸»åŠ›å‡€æµå‡ºæ ·æœ¬: {len(df)-positive_count:,} ({(len(df)-positive_count)/len(df)*100:.1f}%)")
    
    # èµ„é‡‘æµå‘å¼ºåº¦åˆ†å¸ƒ
    strong_inflow = (df['netflow_amount_main'] > df['netflow_amount_main'].quantile(0.75)).sum()
    strong_outflow = (df['netflow_amount_main'] < df['netflow_amount_main'].quantile(0.25)).sum()
    logging.info(f"\nèµ„é‡‘æµå‘å¼ºåº¦:")
    logging.info(f"  - å¼ºæµå…¥ï¼ˆå‰25%ï¼‰: {strong_inflow:,} ({strong_inflow/len(df)*100:.1f}%)")
    logging.info(f"  - å¼ºæµå‡ºï¼ˆå25%ï¼‰: {strong_outflow:,} ({strong_outflow/len(df)*100:.1f}%)")
    
    logging.info(f"\n" + "=" * 80)
    logging.info(f"âœ“ èµ„é‡‘æµæ•°æ®ä¸‹è½½å®Œæˆ")
    logging.info(f"=" * 80)
    
    return df


if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description='ä¸‹è½½èµ„é‡‘æµå‘æ•°æ®')
    parser.add_argument('--days', type=int, default=365, help='ä¸‹è½½å¤©æ•°èŒƒå›´ï¼ˆé»˜è®¤365=1å¹´ï¼‰')
    parser.add_argument('--output', type=str, default='data/raw', help='è¾“å‡ºç›®å½•')
    
    args = parser.parse_args()
    
    download_moneyflow(days_range=args.days, output_dir=args.output)
